[
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Getting started",
    "section": "",
    "text": "MATLAB ‚â•R2024a\nkneighbor: Statistics and Machine Learning Toolbox (if method=\"indirect\")\nmumap: Manopt (if solver=\"trustregions\") or Deep Learning Toolbox (if solver=\"adam\")\nmumap: Parallel Computing Toolbox (if gpu=true)\n\n\n\n\nAdd ./abct-matlab/abct to your MATLAB path.\naddpath('./abct-matlab/abct')\n\n\n\nInstall using the MATLAB Package Manager.\nmpminstall(\"./abct-matlab/abct\")\n\n\n\nBasic usage:\n[output1, output2, ...] = function_name(input1, input2, ..., name=value)\nSee the individual function pages for more details.\nSee the individual examples for example usage (in Python).\n\n\n\n\n\n\n\nPython ‚â•3.11\nnumpy\nscipy\nigraph\npytorch\npymanopt\npynndescent\npydantic\n\n\n\n\nDownload the latest release and install using pip.\npip install ./abct-python\n\n\n\nInstall directly from pypi (may not be the latest release).\npip install abct\n\n\n\nBasic usage:\nimport abct\noutput1, output2, ... = abct.function_name(input1, input2, ..., name=value)\nSee the individual function pages for more details.\nSee the individual examples for example usage.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "installation.html#matlab",
    "href": "installation.html#matlab",
    "title": "Getting started",
    "section": "",
    "text": "MATLAB ‚â•R2024a\nkneighbor: Statistics and Machine Learning Toolbox (if method=\"indirect\")\nmumap: Manopt (if solver=\"trustregions\") or Deep Learning Toolbox (if solver=\"adam\")\nmumap: Parallel Computing Toolbox (if gpu=true)\n\n\n\n\nAdd ./abct-matlab/abct to your MATLAB path.\naddpath('./abct-matlab/abct')\n\n\n\nInstall using the MATLAB Package Manager.\nmpminstall(\"./abct-matlab/abct\")\n\n\n\nBasic usage:\n[output1, output2, ...] = function_name(input1, input2, ..., name=value)\nSee the individual function pages for more details.\nSee the individual examples for example usage (in Python).",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "installation.html#python",
    "href": "installation.html#python",
    "title": "Getting started",
    "section": "",
    "text": "Python ‚â•3.11\nnumpy\nscipy\nigraph\npytorch\npymanopt\npynndescent\npydantic\n\n\n\n\nDownload the latest release and install using pip.\npip install ./abct-python\n\n\n\nInstall directly from pypi (may not be the latest release).\npip install abct\n\n\n\nBasic usage:\nimport abct\noutput1, output2, ... = abct.function_name(input1, input2, ..., name=value)\nSee the individual function pages for more details.\nSee the individual examples for example usage.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "functions/shrinkage.html",
    "href": "functions/shrinkage.html",
    "title": "abct",
    "section": "",
    "text": "SHRINKAGE Network shrinkage\n\n   X1 = shrinkage(X)\n\n   Inputs:\n       X:  Network matrix of size n x n\n\n   Outputs:\n       X1: Shrunken network matrix.\n\n   Methodological notes:\n       The shrinkage algorithm uses cubic interpolation to \"despike\" an\n       initial peak in the eigenspectrum.\n\n   See also:\n       RESIDUALN.",
    "crumbs": [
      "Functions",
      "Shrinkage"
    ]
  },
  {
    "objectID": "functions/mumap.html",
    "href": "functions/mumap.html",
    "title": "abct",
    "section": "",
    "text": "MUMAP m-umap low-dimensional embedding\n\n   U = mumap(X);\n   [U, Partition, CostHistory] = mumap(X, Name=Value);\n\n   Inputs:\n       W:  Network matrix of size n x n.\n\n       OR\n\n       X:  Data matrix of size n x p, where\n           n is the number of data points and\n           p is the number of features.\n\n       Name=[Value] Arguments:\n           d=[Embedding dimension].\n               Positive integer (default is 3).\n\n           kappa=[Number of nearest neighbors].\n               Positive integer (default is 30).\n\n           alpha=[Inverse amplitude of long-range attraction].\n               Positive scalar &gt;= 1 (default is 1).\n               Larger alpha implies weaker long-range attraction.\n\n           beta=[Slope of long-range attraction].\n               Positive scalar &lt;= 1 (default is 1).\n               Larger beta implies faster decay of attraction.\n\n           gamma=[Modularity resolution parameter].\n               Positive scalar (default is 1).\n\n           similarity=[Type of similarity].\n               \"network\": Network connectivity (default).\n               \"corr\": Pearson correlation coefficient.\n               \"cosim\": Cosine similarity.\n\n           method=[Method of nearest-neighbor search].\n               \"direct\": Direct computation of similarity matrix.\n               \"indirect\": knnsearch (MATLAB) or pynndescent (Python).\n\n           replicates=[Number of modularity replicates].\n               Positive integer (default is 10).\n\n           finaltune=[Modularity final tuning].\n               Logical scalar (default is true).\n\n           partition=[Module partition].\n               Integer vector: module partition of length n (default is []).\n\n           start=[Initial embedding method].\n               \"greedy\": Spherical maximin initialization (default).\n               \"spectral\": Spectral initialization of module (k x k) matrix.\n               \"spectral_nn\": Spectral initialization of full (n x n) matrix.\n               Numeric matrix: Initial embedding of size n x d, where:\n                   n is the number of data points and\n                   d is the embedding dimension.\n\n           solver=[Optimization solver].\n               \"trustregions\": Manopt trust-regions method (default for MATLAB).\n               \"adam\": Adaptive Moment Estimation optimizer (default for Python).\n\n           maxiter=[Maximum number of iterations].\n               Positive integer (default is 10000).\n\n           learnrate=[Optimizer learning rate].\n               Positive scalar (default is 0.001).\n\n           tolerance=[Solution tolerance].\n               Positive scalar (default is 1e-6).\n\n           gpu=[Use GPU].\n               Logical (default is false).\n\n           cache=[Cache gradient matrices].\n               Logical (default is false).\n\n           verbose=[Verbose output].\n               Logical (default is true).\n\n   Outputs:\n       U:  Embedding matrix of size n x d.\n       Partition: Module partition of length n.\n       CostHistory: Cost history of optimization.\n\n   Methodological notes:\n       m-umap is a first-order approximation of the true parametric\n       loss of UMAP, with spherical constraints. It is simultaneously\n       equivalent to the modularity, and to spring layout methods with\n       Cauchy components and spherical constraints.\n\n   Dependencies:\n       MATLAB: \n           Statistics and Machine Learning Toolbox (if method=\"indirect\")\n           Deep Learning Toolbox (if solver=\"adam\")\n           Manopt (if solver=\"trustregions\")\n       Python: \n           igraph\n           PyTorch\n           PyNNDescent (if method=\"indirect\")\n           PyManopt (if solver=\"trustregions\") * warning: slow *\n\n   See also:\n       KNEIGHBOR, LOUVAINS, KNEICOMP",
    "crumbs": [
      "Functions",
      "Mumap"
    ]
  },
  {
    "objectID": "functions/louvains.html",
    "href": "functions/louvains.html",
    "title": "abct",
    "section": "",
    "text": "LOUVAINS Efficient Louvain modularity maximization of sparse networks (MATLAB)\n LEIDEN igraph Leiden modularity maximization (Python)\n\n   [M, Q] = louvains(W, Name=Value)        % MATLAB\n   [M, Q] = leiden(W, Name=Value)          # Python\n\n   Inputs:\n       W:  Network matrix of size n x n.\n\n       Name=[Value] Arguments:\n\n           gamma=[Resolution parameter].\n               Positive scalar (default is 1).\n\n           start=[Initial module assignments].\n               Vector of length n (default is 1:n).\n\n           replicates=[Number of replicates].\n               Positive integer (default is 10).\n\n           finaltune=[Final tuning of optimized assignment].\n               Logical (default is false).\n\n           tolerance=[Convergence tolerance].\n               Positive scalar (default is 1e-10).\n\n           display=[Display progress].\n               \"none\": no display (default).\n               \"replicate\": display progress at each replicate.\n\n   Outputs:\n       M: Vector of module assignments (length n).\n       Q: Value of maximized modularity.\n\n   See also:\n       MUMAP, LOYVAIN.",
    "crumbs": [
      "Functions",
      "Louvains"
    ]
  },
  {
    "objectID": "functions/kneicomp.html",
    "href": "functions/kneicomp.html",
    "title": "abct",
    "section": "",
    "text": "KNEICOMP Components of neighbor matrices\n\n   V = kneicomp(W, k)\n   V = kneicomp(X, k)\n   V = kneicomp(_, k, weight)\n   V = kneicomp(_, k, weight, Name=Value)\n\n   Inputs:\n       W: Network matrix of size n x n.\n       OR\n       X: Data matrix of size n x p, where\n           n is the number of data points and\n           p is the number of features.\n\n       k: Number of components.\n\n       weight: Type of components\n           \"weighted\": Weighted components (default).\n           \"binary\": Binary components.\n\n       Name=[Value] Arguments:\n           KNEIGHBOR: type, kappa, similarity, method\n               (see KNEIGHBOR for details).\n           LOYVAIN: All Name=Value arguments\n               (binary components only, see LOYVAIN for details).\n\n   Outputs:\n       V: Component matrix (size n x k).\n\n   Methodological notes:\n       By default, weighted components are eigenvectors of\n       common-neighbors matrices. In imaging neuroscience, these\n       components are approximately equivalent to co-activity gradients\n       (diffusion-map embeddings).\n \n       Correspondingly, binary components are modules of common-neighbors \n       matrices, estimated using the Loyvain algorithm. They are\n       equivalent to eigenvectors of common-neighbors matrices with binary\n       constraints. The order of binary components will be arbitrary. \n\n   See also:\n       KNEIGHBOR, LOYVAIN, MUMAP.",
    "crumbs": [
      "Functions",
      "Kneicomp"
    ]
  },
  {
    "objectID": "functions/degree.html",
    "href": "functions/degree.html",
    "title": "abct",
    "section": "",
    "text": "DEGREE Degree of network matrix\n\n   S = degree(W)\n   S = degree(W, type)\n\n   Inputs:\n       W: Network matrix of size n x n.\n\n       type: Degree type\n           \"first\": (First) degree (default).\n           \"second\": Second degree.\n           \"residual\": Degree after global residualization.\n\n   Outputs:\n       S: Degree vector (length n).\n\n   Methodological notes:\n       The first degree is the sum of connection weights. The second\n       degree is the sum of squared connection weights. Together, the\n       first and second degrees are exactly or approximately equivalent to\n       several measures of network communication and control.\n\n       The residual degree is the degree after first-component removal and\n       can be approximately equivalent to the primary co-activity gradient\n       in functional MRI co-activity networks.\n\n   See also:\n       RESIDUALN, DISPERSION.",
    "crumbs": [
      "Functions",
      "Degree"
    ]
  },
  {
    "objectID": "functions/canoncov.html",
    "href": "functions/canoncov.html",
    "title": "abct",
    "section": "",
    "text": "CANONCOV Canonical covariance analysis (aka partial least squares)\n          Canonical correlation analysis\n\n   [A, B, U, V, R] = canoncov(X, Y, k)\n   [A, B, U, V, R] = canoncov(X, Y, k, type)\n   [A, B, U, V, R] = canoncov(X, Y, k, type, resid)\n   [A, B, U, V, R] = canoncov(X, Y, k, type, resid, corr)\n   [A, B, U, V, R] = canoncov(X, Y, k, type, resid, corr, Name=Value)\n\n   Inputs:\n       X: Data matrix of size n x p, where\n          n is the number of data points and\n          p is the number of features.\n\n       Y: Data matrix of size n x q, where\n          n is the number of data points and\n          q is the number of features.\n\n       k: Number of canonical components (positive integer).\n\n       type: Weighted or binary canonical analysis.\n           \"weighted\": Weighted canonical analysis (default).\n           \"binary\": Binary canonical analysis.\n\n       resid: Global residualization (logical scalar).\n           0: No global residualization.\n           1: Global residualization via degree correction (default).\n\n       corr: Canonical correlation analysis (logical scalar).\n           0: Canonical covariance analysis (default).\n           1: Canonical correlation analysis.\n\n       Name=[Value] Arguments\n           (binary canonical analysis only):\n           See LOYVAIN for all Name=Value options.\n\n   Outputs:\n       A: Canonical coefficients of X (size p x k).\n       B: Canonical coefficients of Y (size q x k).\n       U: Canonical components of X (size n x k).\n       V: Canonical components of Y (size n x k).\n       R: Canonical covariances or correlations (size k x k).\n          If type is \"weighted\", R denotes the actual covariances or\n          correlations. If type is \"binary\", R denotes the\n          normalized covariances or correlations.\n\n   Methodological notes:\n       Weighted canonical correlation or covariance analysis is computed\n       via singular value decomposition of cross-covariance matrix.\n\n       Binary canonical covariance analysis is computed via co-Loyvain\n       k-means clustering of cross-covariance matrix. This analysis\n       produces binary orthogonal canonical coefficients.\n\n       Binary canonical covariance analysis is computed via co-Loyvain\n       k-means clustering of _whitened_ cross-covariance matrix. This\n       analysis produces binary orthogonal canonical coefficients for\n       the whitened matrix. However, the output coefficients after\n       dewhitening will, in general, not be binary.\n\n       Global residualization is implemented via generalized degree\n       correction, and converts k-means co-clustering into k-modularity\n       co-maximization.\n\n   See also:\n       COLOYVAIN, LOYVAIN, RESIDUALN.",
    "crumbs": [
      "Functions",
      "Canoncov"
    ]
  },
  {
    "objectID": "examples/7_dispersion.html",
    "href": "examples/7_dispersion.html",
    "title": "Dispersion Centralities",
    "section": "",
    "text": "Open in Colab\nThe squared coefficient of variation is a basic measure of normalized dispersion. It is defined as the ratio of the variance over the squared mean, or equivalently, the ratio of the first and second degrees.\nHere, we show that in networks with relatively homogeneous connections within modules, the squared coefficient of variation is equivalent to a variant of the participation coefficient, a popular module-based measure of connectional diversity. It is specifically equivalent to the k-participation coefficient, the participation coefficient normalized by module size. We show these equivalences in structural and correlation co-neighbor networks from our example brain-imaging data, because these networks have high connectional homogeneity by construction.\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn\n\n# Import modules\nimport abct\nimport numpy as np\nfrom abct_utils import W, C, ordw, ordc, fig_scatter, fig_surf, fig_imshow\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCompute and visualize co-neighbor matrices\nWe first compute and visualize the co-neighbor matrices.\n\n# Get co-neighbor matrices\nWn = abct.kneighbor(W, \"common\", kappa=0.1).toarray()\nCn = abct.kneighbor(C, \"common\", kappa=0.1).toarray()\n\n# Visualize co-neighbor matrices\nfig_imshow(Wn[np.ix_(ordw, ordw)],\n          \"Structural co-neighbor network\",\n          \"inferno\").show()\n\nfig_imshow(Cn[np.ix_(ordc, ordc)],\n          \"Correlation co-neighbor network\",\n          \"viridis\").show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nCompute squared coefficient of variation and k-participation coefficient\nWe now compute the squared coefficient of variation and the k-participation coefficient for these co-neighbor networks. Note that the k-participation coefficient is defined for a specific module partition, and we therefore compute it for a range of partitions.\n\n# Get squared coefficient of variation\nWV = abct.dispersion(Wn, \"coefvar2\")\nCV = abct.dispersion(Cn, \"coefvar2\")\n\n# Get participation coefficients\nK = np.arange(5, 30, 5)    # number of clusters\nrepl = 10                  # number of replicates\n\n# Set random seed\nnp.random.seed(1)\n\n### Run Loyvain k-modularity\nWP = [None] * len(K)\nCP = [None] * len(K)\nfor i, k in enumerate(K):\n    print(f\"Number of clusters: {k}\")\n    Mw = abct.loyvain(Wn, k, \"kmodularity\", replicates=repl)[0]\n    Mc = abct.loyvain(Cn, k, \"kmodularity\", replicates=repl)[0]\n    WP[i] = abct.dispersion(Wn, \"kpartcoef\", Mw)\n    CP[i] = abct.dispersion(Cn, \"kpartcoef\", Mc)\n\nNumber of clusters: 5\nNumber of clusters: 10\nNumber of clusters: 15\nNumber of clusters: 20\nNumber of clusters: 25\n\n\n\n\nShow maps of the squared coefficient of variation\nWe next show the maps of the squared coefficient of variation, separately for the structural and correlation co-neighbor networks.\n\ncv2s = {\"Structural - (Squared coefficient of variation)\": (- WV, \"inferno\"),\n        \"Correlation - (Squared coefficient of variation)\": (- CV, \"viridis\")}\n\nfor i, (name, vals_cmap) in enumerate(cv2s.items()):\n    vals, cmap = vals_cmap\n    fig_surf(vals, name, cmap)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plots of squared coefficient of variation and k-participation coefficient\nFinally, we show the scatter plots of the squared coefficient of variation and the k-participation coefficient, separately for the structural and correlation co-neighbor networks. As expected, the squared coefficient of variation and the k-participation coefficient are strongly correlated, and this correlation increases with the number of modules, as the within-module connectivity becomes more homogeneous.\n\nnormalize = lambda x: (x - x.min()) / (x.max() - x.min())\n\nfor i in range(len(K)):\n    if i == 0:\n        fig = fig_scatter(- np.log10(1 - WP[i]),\n                            normalize(- WV))\n    else:\n        fig.add_scatter(x = - np.log10(1 - WP[i]),\n                        y = normalize(- WV), \n                        mode=\"markers\")\n\nr = np.corrcoef(WV, np.array(WP))[0][1:]\nfig.update_layout(xaxis_title=\"log-rescaled k-participation coefficient\",\n                  yaxis_title=\"rescaled - (Squared coefficient of variation)\",\n                  title=f\"Structural network: r ~ {-np.mean(r):.3f}\",\n                  showlegend=False).show()\n\nfor i in range(len(K)):\n    if i == 0:\n        fig = fig_scatter(- np.log10(1 - CP[i]), \n                            normalize(- CV))\n    else:\n        fig.add_scatter(x = - np.log10(1 - CP[i]), \n                              y = normalize(- CV), \n                              mode=\"markers\")\n\nr = np.corrcoef(CV, np.array(CP))[0][1:]\nfig.update_layout(xaxis_title=\"log-rescaled k-participation coefficient\",\n                  yaxis_title=\"rescaled - (Squared coefficient of variation)\",\n                  title=f\"Correlation network: r ~ {-np.mean(r):.3f}\",\n                  showlegend=False).show()",
    "crumbs": [
      "Examples",
      "Dispersion Centralities"
    ]
  },
  {
    "objectID": "examples/5_mumap.html",
    "href": "examples/5_mumap.html",
    "title": "m-umap Embeddings",
    "section": "",
    "text": "Open in Colab\nUMAP is a prominent manifold-learning method especially popular in the analysis and visualization of single-cell, population-genetic, and other biological data. Here, we consider the performance of the first-order approximation of (the true parametric) UMAP objective on our example brain-imaging data. We term this approximation m-umap, and note that it reduces to a generalized modularity.\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn\n\n# Import modules\nimport abct\nimport numpy as np\nfrom abct_utils import C, ordc, fig_scatter3, fig_scatter, fig_surf, fig_imshow\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nVisualize nearest-neighbor networks\nUMAP and m-umap ultimately seek to approximate symmetric ùúÖ-nearest-neighbor representations. We first visualize the structure of nearest-neighbor correlation networks in our example data.\n\n# Correlation nearest-neighbor network\nCn = abct.kneighbor(C, \"nearest\", 50).toarray()\n\nfig_imshow(Cn[np.ix_(ordc, ordc)],\n           \"Correlation nearest-neighbor network\",\n           \"viridis\").show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\nGet m-umap embeddings\nSimple use of m-umap can lead to runaway solutions. Here, we check this outcome by embedding m-umap solutions on (k-dimensional) spheres. These embeddings have an additional nice property of reducing m-umap with binary constraints to the standard modularity.\nWe developed a simple algorithm to optimize m-umap. First, we optimized the binary m-umap via modularity maximization of the symmetric ùúÖ-nearest-neighbor network. Next, we used the resulting module-indicator matrix to initialize the continuous embedding. Finally, we optimized the continuous embedding directly on the sphere. We now use this algorithm to detect m-umap embeddings.\n\nnp.random.seed(1)\nU3, M, _ = abct.mumap(C, kappa=50, learnrate=0.01, verbose=False)\n\n\n\nVisualize m-umap embeddings on a sphere\nWe now show the three-dimensional m-umap embeddings. Each point represents a node, and colors represent module affiliations (aka binary m-umap embeddings). We also use a Mercator (classic-map) projection to project these embeddings onto a plane.\n\nX3, Y3, Z3 = zip(*U3)\nfig = fig_scatter3(X3, Y3, Z3, f\"m-umap embedding on a sphere\")\nfig.update_traces(marker=dict(color=M+1), marker_showscale=True)\nfig.show()\n\nU2 = abct.muma.projection(U3)\nX2, Y2 = zip(*U2)\nfig = fig_scatter(X2, Y2, \"\", \"\", f\"m-umap projection on a plane\")\nfig.update_traces(marker=dict(color=M+1), marker_showscale=True)\nfig.show()\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nShow maps of individual modules\nFinally, we show the maps of the individual binary m-umap embeddings.\n\nfor i in range(M.max()+1):\n    fig_surf(1.0 * (M == i), f\"Module {i+1}\", \"viridis\")",
    "crumbs": [
      "Examples",
      "m-umap Embeddings"
    ]
  },
  {
    "objectID": "examples/3_canoncov.html",
    "href": "examples/3_canoncov.html",
    "title": "Canonical Components",
    "section": "",
    "text": "Open in Colab\nCanonical covariance analysis (aka partial least squares correlation) finds linear relationships between pairs of datasets that have the same number of data points but usually have different numbers of features. It specifically finds k pairs of coefficient vectors that transform the original data points into k pairs of components, in a way that maximizes the total covariance over all pairs of components.\nThis method is popular across diverse scientific fields, but its application to data with many features can often lead to unstable estimates of coefficients. This problem can be ameliorated, to some extent, through the adoption of sparse variants of these methods.\nHere, we extend the Loyvain method to do sparse binary canonical covariance analysis. We illustrate this method for finding the cross-correlation structure of structural and correlation networks from our example brain-imaging data.\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn\n\n# Import modules\nimport abct\nimport numpy as np\nfrom abct_utils import W, C, fig_scatter, fig_surf\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nRun canonical covariance analysis\nA common formulation of canonical covariance analysis centers on the detection of (principal) components of cross-covariance matrices. We build on this formulation to extend the Loyvain algorithm to do a binary variant of this analysis by independently clustering the rows and columns of cross-covariance matrices, or any other bipartite (two-part) networks, for that matter. This process simultaneously finds pairs of modules from both datasets and is equivalent to canonical covariance analysis with binary coefficients.\n\n# Number of canonical components\nk = 5\n\n# Weighted canonical covariance analysis (with degree correction by default)\nnp.random.seed(1)\nA_wei, B_wei, U_wei, V_wei, R_wei = abct.canoncov(W, C, k, \"weighted\")\n\n# Reverse the signs of the coefficients\n# (signs of canonical coefficients are arbitrary)\nA_wei = - A_wei\nB_wei = - B_wei\nU_wei = - U_wei\nV_wei = - V_wei\n\n# Binary canonical covariance analysis (with degree correction by default)\nA_bin, B_bin, U_bin, V_bin, R_bin = abct.canoncov(W, C, k, \"binary\")\n\n\n\nShow maps of weighted and binary canonical coefficients and components\nWe now visualize maps of the weighted and binary canonical coefficients and components. These results show that binary coefficients can be sparse and more interpretable than their weighted counterparts. Moreover, binary coefficients lead to a particularly simple definition of canonical components, as sums of data points over the non-zero features.\n\nccas = {\"Weighted structural canonical coefficient\": (A_wei[:, 0], \"inferno\"),\n        \"Weighted structural canonical component\": (U_wei[:, 0], \"inferno\"),\n        \"Weighted functional canonical coefficient\": (B_wei[:, 0], \"viridis\"),\n        \"Weighted functional canonical component\": (V_wei[:, 0], \"viridis\"),\n        \"Binary structural canonical coefficient\": (A_bin[:, 0], \"inferno\"),\n        \"Binary structural canonical component\": (U_bin[:, 0], \"inferno\"),\n        \"Binary functional canonical coefficient\": (B_bin[:, 0], \"viridis\"),\n        \"Binary functional canonical component\": (V_bin[:, 0], \"viridis\")}\n\nfor i, (name, vals_cmap) in enumerate(ccas.items()):\n    vals, cmap = vals_cmap\n    fig_surf(vals, name, cmap)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize scatters of canonical covariances\nWe now visualize the normalized covariances between the first five canonical components from the weighted and binary canonical covariance analyses. Note that the values of these covariances are not directly comparable due to different normalizations of the weighted and binary analyses.\n\nfig_scatter(np.arange(k), R_wei, \n            \"Canonical components\", \n            \"Canonical covariances\", \n            \"Weighted canonical covariance analysis\").show()\n\nfig_scatter(np.arange(k), R_bin, \n            \"Canonical components\", \n            \"Canonical covariances\", \n            \"Binary canonical covariance analysis\").show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nVisualize scatters of canonical components\nFinally, we directly show the relationship between the first weighted and binary canonical components. These results show generally high correlations between these components. We note, however, that these high correlations are not necessarily guaranteed because binary constraints can, in principle, result in considerably different components.\n\n# Scatter plot of structural canonical components\nrs = np.corrcoef(U_wei[:, 0], U_bin[:, 0])[0, 1]\nfig_scatter(U_wei[:, 0], U_bin[:, 0],\n            \"Weighted canonical components\",\n            \"Binary canonical components\",\n           f\"Structural canon. comp. (r = {rs:.3f})\").show()\n\nrf = np.corrcoef(V_wei[:, 0], V_bin[:, 0])[0, 1]\nfig_scatter(V_wei[:, 0], V_bin[:, 0],\n            \"Weighted canonical components\",\n            \"Binary canonical components\",\n           f\"Correlation canon. comp. (r = {rf:.3f})\").show()",
    "crumbs": [
      "Examples",
      "Canonical Components"
    ]
  },
  {
    "objectID": "examples/1_residualn.html",
    "href": "examples/1_residualn.html",
    "title": "Global Residualization",
    "section": "",
    "text": "Open in Colab\nMany datasets and networks contain dominant global patterns that sometimes represent artifactual, trivial, or irrelevant structure. Correspondingly, analyses often seek to remove these patterns to uncover or accentuate interesting underlying structure. We use the term global residualization to describe this removal.\nHere, we show approximate equivalence between three variants of global residualization across unsupervised learning, network science, and imaging neuroscience:\n\nFirst-component removal: Subtraction of rank-one approximation of the data (common in unsupervised learning).\nDegree correction: Subtraction of the normalized outer product of node degree vectors (common in network science).\nGlobal signal regression: Regression of the mean time series signal from the time series data (common in imaging neuroscience).\n\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn\n\n# Import modules\nimport abct\nimport numpy as np\nfrom abct_utils import W, X, C, ordw, ordc, not_eye, fig_scatter, fig_imshow\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nShow original structural and correlation networks\nTo show these relationships, we first consider the original structural and correlation networks in our example brain-imaging data. Each network contains 360 nodes (rows and columns) that denote cortical (brain) regions.\n\nfig_imshow(W[np.ix_(ordw, ordw)],\n           \"Original structural network\",\n           \"inferno\").show()\n\nfig_imshow(C[np.ix_(ordc, ordc)],\n           \"Original correlation network\",\n           \"viridis\").show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nResidualize structural and correlation networks\nWe now apply the three variants of global residualization to these networks. Note that while the degree and first component are removed directly from the networks, the global signal is regressed out of the time series data.\n\n# Residualize structural network\nWd = abct.residualn(W, \"degree\")\nWr = abct.residualn(W, \"rankone\")\n\n# Residualize correlation networks\nCd = abct.residualn(C, \"degree\")\nXg = abct.residualn(X.T, \"global\").T\nXg = Xg / np.linalg.norm(Xg, axis=0, keepdims=True)\nCg = Xg.T @ Xg\n\n\n\nVisualize residual structural and correlation networks\nWe now visualize variants of the residual structural and correlation networks.\n\nfig_imshow(Wd[np.ix_(ordw, ordw)],\n           \"Struct. degree correction\",\n           \"inferno\").show()\n\nfig_imshow(Wr[np.ix_(ordw, ordw)],\n           \"Struct. first-component removal\",\n           \"inferno\").show()\n\nfig_imshow(Cd[np.ix_(ordc, ordc)],\n           \"Corr. degree correction\",\n           \"viridis\").show()\n\nfig_imshow(Cg[np.ix_(ordc, ordc)],\n           \"Corr. global signal regression\",\n           \"viridis\").show()\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\nFinally, we directly show the strong similarity between network weights after distinct residualizations.\n\nr = np.corrcoef(Wr[not_eye], Wd[not_eye])[0, 1]\nfig_scatter(Wr[not_eye], Wd[not_eye], \n            \"Weights after first-component removal\",\n            \"Weights after degree correction\",\n            f\"Structural network (r = {r:.3f})\").show()\n\nr = np.corrcoef(Cg[not_eye], Cd[not_eye])[0, 1]\nfig_scatter(Cg[not_eye], Cd[not_eye], \n            \"Weights after global-signal regression\",\n            \"Weights after degree correction\",\n            f\"Correlation network (r = {r:.3f})\").show()",
    "crumbs": [
      "Examples",
      "Global Residualization"
    ]
  },
  {
    "objectID": "examples/2_loyvain.html",
    "href": "examples/2_loyvain.html",
    "title": "Loyvain Clustering",
    "section": "",
    "text": "Open in Colab\nThe Loyvain method is a fusion of Lloyd and Louvain methods, two classic algorithms for k-means clustering and modularity maximization, respectively. This method specifically interpolates between Lloyd and Louvain, and thus represents a unified algorithm for k-means clustering and modularity maximization.\nHere, we compare the Loyvain method against classic k-means and spectral clustering methods on our example brain-imaging correlation networks.",
    "crumbs": [
      "Examples",
      "Loyvain Clustering"
    ]
  },
  {
    "objectID": "examples/2_loyvain.html#visualize-clustering-results",
    "href": "examples/2_loyvain.html#visualize-clustering-results",
    "title": "Loyvain Clustering",
    "section": "Visualize clustering results",
    "text": "Visualize clustering results\nWe now show the maps of k-modularity clusters detected with the Loyvain method.\n\nfor i, Mi in enumerate(MKmod):\n    fig_surf(Mi, f\"Loyvain on correlation network: {K[i]} modules\",\n             \"tab20b\", pmin=0, pmax=100)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare clustering similarities\nFinally, we directly compare the clustering similarities of the Loyvain method to standard k-means and spectral clustering algorithms.\n\n# Scatter plots of k-means clustering similarities\nx, y = kmeans_lloyd_similarity, kmeans_loyvain_similarity\nfig = fig_scatter(x, y,\n                  \"Lloyd k-means\",\n                  \"Loyvain k-means\",\n                  \"K-means similarity\")\nfig.add_shape(type=\"line\",\n              x0=np.min([x, y]),\n              y0=np.min([x, y]),\n              x1=np.max([x, y]),\n              y1=np.max([x, y]))\nfig.show()\n\n# Scatter plots of spectral clustering similarities\nx, y = spectral_shimalik_similarity, spectral_loyvain_similarity\nfig = fig_scatter(x, y,\n                 \"Lloyd spectral\",\n                 \"Loyvain spectral\",\n                 \"Spectral similarity\")\nfig.add_shape(type=\"line\",\n              x0=np.min([x, y]),\n              y0=np.min([x, y]),\n              x1=np.max([x, y]),\n              y1=np.max([x, y]))\nfig.show()",
    "crumbs": [
      "Examples",
      "Loyvain Clustering"
    ]
  },
  {
    "objectID": "examples/4_kneicomp.html",
    "href": "examples/4_kneicomp.html",
    "title": "Co-neighbor Components and Residual Degree",
    "section": "",
    "text": "Open in Colab\nDiffusion-map embedding is a versatile method for nonlinear dimensionality reduction. One variant of this method has transformed analyses in much of imaging neuroscience by robustly detecting co-activity (functional) gradients, low-dimensional representations of correlation networks that capture important properties of functional organization.\nHere, we show an approximate equivalence between the components of co-neighbor networks ‚Äî a simple class of integer networks ‚Äî and this variant of diffusion-map embedding in our example brain-imaging data.\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn\n\n# Import modules\nimport abct\nimport numpy as np\nfrom abct_utils import W, C, ordc, fig_scatter, fig_surf, fig_imshow\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nVisualize co-neighbor networks\nCo-neighbor networks, as their name suggests, encode the number of shared ùúÖ-nearest, or strongest correlated, neighbors between pairs of nodes. We first visualize the structure of co-neighbor correlation networks in our data.\n\n# Define and visualize co-neighbor networks\n# (kappa = 0.1 is equivalent to the top 10% nearest neighbors)\nCn = abct.kneighbor(C, \"common\", 0.1).toarray()\n\nfig_imshow(Cn[np.ix_(ordc, ordc)],\n           \"Correlation co-neighbor network\",\n           \"viridis\").show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\nGet co-activity gradients\nNext, we compute the components of co-neighbor networks.\n\nnp.random.seed(1)\n\n# Define co-activity gradient parameters (see kneighbor)\nk = 5\nkwargs = {\"type\":\"common\", \"kappa\":0.1, \"similarity\":\"network\"}\n\n# Weighted co-activity gradients\nV_wei = abct.kneicomp(C, k, \"weighted\", **kwargs)\nV_wei = V_wei[:, [0, 1, 3]] # match components to standard order\n\n# Binary co-activity gradients\nV_bin = abct.kneicomp(C, k, \"binary\", **kwargs)\nV_bin = V_bin[:, [1, 2, 3]] # match components to standard order\n\n# Flip sign of weighted gradients to match binary gradients\nV_wei *= np.sign(np.sum(V_wei * V_bin, 0))\n\n\n\nShow maps of weighted and binary co-activity components\nWe now show the maps of three weighted and binary co-activity components. These maps closely resemble the maps of co-activity gradients estimated with diffusion-map embedding.\n\ncomps = {\"Weighted co-activity gradient\": (V_wei, \"viridis\"),\n        \"Binary co-activity gradient\": (V_bin, \"viridis\")}\n\nfor i, (name, Vals_cmap) in enumerate(comps.items()):\n    Vals, cmap = Vals_cmap\n    for j in range(Vals.shape[1]):\n        fig_surf(Vals[:, j], f\"{name} {j+1}\", cmap)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow map and scatter of residual degree\nThe primary co-activity gradient plays an especially important role in imaging neuroscience because it represents a transition between primary and association cortical areas. We conclude by showing a particular simple approximation of this component, as the degree of the residual network after first-component removal, or global signal regression.\n\n# Residual degree\nD_wei = abct.degree(C, \"residual\")\n\n# Map of residual degree\nfig_surf(D_wei, \"Residual degree\", \"viridis\")\n\n# Scatter of residual degree and weighted co-activity gradient\nr = np.corrcoef(D_wei, V_wei[:, 0])[0, 1]\nfig = fig_scatter(D_wei, V_wei[:, 0], \n                 \"Residual degree\", \n                 \"Weighted co-activity gradient\", \n                f\"Correlation network (r = {r:.3f})\").show()",
    "crumbs": [
      "Examples",
      "Co-neighbor Components"
    ]
  },
  {
    "objectID": "examples/6_degree.html",
    "href": "examples/6_degree.html",
    "title": "Degree Centralities",
    "section": "",
    "text": "Open in Colab\nDegree is a basic measure of node centrality, defined as the sum of node connection weights. Here, we show the equivalence of the degree and its extension, the second degree, with five measures of communication, control, and diversity. Since many of these measures assume that activity propagates on physical networks, we primarily study them on structural networks in our example brain-imaging data.\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn nctpy bctpy\n\n# Import modules\nimport abct\nimport numpy as np\nfrom scipy import sparse\nfrom nctpy import utils, metrics\nfrom bct import mean_first_passage_time\nfrom abct_utils import W, fig_scatter, fig_surf\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nRelationship between degree and measures of diffusion\nWe first consider the weighted degree (or strength), defined for each node as the sum of its connection weights. We compute and visualize the map of the degree.\n\nDegree = abct.degree(W, \"first\")\nfig_surf(Degree, \"Degree\", \"inferno\").show()\n\n/tmp/ipykernel_300480/2920194165.py:2: UserWarning:\n\nFigureCanvasAgg is non-interactive, and thus cannot be shown\n\n\n\n\n\n\n\n\n\n\nIn many cases of interest, the degree is approximately equivalent to eigenvector centrality and diffusion efficiency, two popular measures of nodal centrality based on the assumption of diffusion dynamics or random walks.\n\nEigenvector_centrality = sparse.linalg.eigs(W, k=1)[1].real.ravel()\nEigenvector_centrality *= np.sign(np.sum(Eigenvector_centrality * Degree))\n\nDiffusion_efficiency = 1/mean_first_passage_time(W).mean(0)\n\nr = np.corrcoef(Degree, Eigenvector_centrality)[0, 1]\nfig = fig_scatter(Degree, Eigenvector_centrality, \n                 \"Degree\", \n                 \"Eigenvector centrality\", \n                f\"Structural network (r = {r:.3f})\").show()\n\nr = np.corrcoef(Degree, Diffusion_efficiency)[0, 1]\nfig = fig_scatter(Degree, Diffusion_efficiency, \n                 \"Degree\", \n                 \"Diffusion efficiency\", \n                f\"Structural network (r = {r:.3f})\").show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nRelationship between second degree and measures of communication and control\nWe next consider the second degree, defined for each node as the sum of its squared connection weights. We compute and visualize the map of the second degree.\n\nSecond_degree = abct.degree(W, \"second\")\nfig_surf(Second_degree,\n        \"Second degree\",\n        \"inferno\").show()\n\n/tmp/ipykernel_300480/1759232829.py:4: UserWarning:\n\nFigureCanvasAgg is non-interactive, and thus cannot be shown\n\n\n\n\n\n\n\n\n\n\nThe second degree is exactly or approximately equivalent to communicability centrality, average controllability, and modal controllability, three popular measures of communication and control. These measures are also based on the assumption of diffusion dynamics and consider the number and length of all possible walks between two pairs of nodes.\n\n# Normalize for communicability analysis\nW_nrm1 = W / sparse.linalg.eigs(W, k=1)[0].real\nCommunicability = np.diag(sparse.linalg.expm(W_nrm1))\n\n# Normalize for controllability analysis\nW_nrm2 = utils.matrix_normalization(W_nrm1, \"discrete\")\nAve_control = metrics.ave_control(W_nrm2, \"discrete\")\nMod_control = metrics.modal_control(W_nrm2)\n\nr = np.corrcoef(Second_degree, Communicability)[0, 1]\nfig = fig_scatter(Second_degree, Communicability, \n                 \"Second degree\", \n                 \"Communicability\", \n                f\"Structural network (r = {r:.3f})\").show()\n\nr = np.corrcoef(Second_degree, Ave_control)[0, 1]\nfig = fig_scatter(Second_degree, Ave_control, \n                 \"Second degree\", \n                 \"Average controllability\", \n                f\"Structural network (r = {r:.3f})\").show()\n\nr = np.corrcoef(Second_degree, Mod_control)[0, 1]\nfig = fig_scatter(Second_degree, Mod_control, \n                 \"Second degree\", \n                 \"Modal controllability\", \n                f\"Structural network (r = {r:.3f})\").show()",
    "crumbs": [
      "Examples",
      "Degree Centralities"
    ]
  },
  {
    "objectID": "examples/8_shrinkage.html",
    "href": "examples/8_shrinkage.html",
    "title": "Network Shrinkage",
    "section": "",
    "text": "Open in Colab\nPopular models in network neuroscience model network growth as a function of spatial proximity and connectional similarity. These models typically start with a sparse ‚Äúseed‚Äù network of connections, and then proceed to simulate growth by adding connections to this network in discrete time steps. Here, we show that the assumption of network growth in a variant of these models can be subsumed by a type of shrinkage, or the weakening of dominant structural patterns in the network. We illustrate this relationship in structural and proximity networks on our example brain-imaging data.\n\nSet up and load data\n\n# Install abct and download abct_utils.py\nbase = \"https://github.com/mikarubi/abct/raw/refs/heads/main\"\n!wget --no-clobber {base}/docs-code/examples/abct_utils.py\n%pip install --quiet abct nilearn\n\n# Import modules\nimport abct\nimport numpy as np\nfrom abct_utils import W, Distance, ordw, not_eye, fig_scatter, fig_imshow\n\nFile ‚Äòabct_utils.py‚Äô already there; not retrieving.\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nVisualize structural networks\nWe begin by visualizing the structural network in our data.\n\nfig_imshow(W[np.ix_(ordw, ordw)], \n            \"Structural network\", \"inferno\",\n            pmin=-0, pmax=-0.01).show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\nCompute proximity networks\nWe now compute and visualize proximity networks and show that these networks accurately approximate structural networks. Here, we define proximity as distance(-Œ±) and simply set Œ± = 1. In practice, we usually fit Œ± to the data, although this has no qualitative effect on our results.\n\n# Get proximity networks\nwith np.errstate(divide=\"ignore\"):\n    Phi = Distance**(-1)\n    np.fill_diagonal(Phi, 0)\n\nfig_imshow(Phi[np.ix_(ordw, ordw)],\n            \"Proximity network\", \"inferno\",\n            pmin=-0, pmax=-0.01).show()\n\nr = np.corrcoef(W[not_eye], Phi[not_eye])[0, 1]\nfig = fig_scatter(W[not_eye], Phi[not_eye],\n                  \"Network weights of structural network\",\n                  \"Network weights of proximity network\",\n                 f\"Structural network (r = {r:.3f})\").show()\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\nScatter plots of connectivity and proximity networks\nWe now sparsify proximity matrices by weakening the contribution of their first several components. This weakening is formally known as shrinkage, and is commonly used to clean covariance or correlation matrices. In our analyses, we find that shrunken proximity networks approximate structural networks with considerably higher accuracy.\n\nPhi_s = abct.shrinkage(Phi)\nr = np.corrcoef(W[not_eye], Phi_s[not_eye])[0, 1]\n\nfig_imshow(Phi_s[np.ix_(ordw, ordw)],\n            \"Shrunken proximity network\", \"inferno\",\n            pmin=-0, pmax=-0.01).show()\n\nr = np.corrcoef(W[not_eye], Phi_s[not_eye])[0, 1]\nfig_scatter(W[not_eye], Phi_s[not_eye],\n                  \"Network weights of structural network\",\n                  \"Network weights of shrunken proximity network\",\n                 f\"Structural network (r = {r:.3f})\").show()",
    "crumbs": [
      "Examples",
      "Network Shrinkage"
    ]
  },
  {
    "objectID": "functions/coloyvain.html",
    "href": "functions/coloyvain.html",
    "title": "abct",
    "section": "",
    "text": "COLOYVAIN K-modularity, k-means, or spectral co-clustering\n\n   [Mx, My, R] = coloyvain(W, k)\n   [Mx, My, R] = coloyvain(W, k, objective)\n   [Mx, My, R] = coloyvain(X, Y, k, objective, similarity)\n   [Mx, My, R] = coloyvain(_, objective, similarity, Name=Value)\n\n   Inputs:\n\n       W: Bipartite network matrix of size p x q.\n\n       X: Data matrix of size n x p, where\n          n is the number of data points and\n          p is the number of features.\n\n       Y: Data matrix of size n x q, where\n          n is the number of data points and\n          q is the number of features.\n\n       k: Number of modules (positive integer).\n\n       objective: Clustering objective.\n           See LOYVAIN for all options.\n\n       similarity: Type of similarity.\n           See LOYVAIN for all options.\n\n       Name=[Value] Arguments.\n           See LOYVAIN for all Name=Value arguments.\n\n   Outputs:\n       Mx: Vector of module assignments for X (length p).\n       My: Vector of module assignments for Y (length q).\n       R: Value of maximized objective.\n\n   Methodological notes:\n       Coloyvain simultaneously clusters X and Y via Loyvain\n       co-clustering of the cross-similarity matrix.\n\n   See also:\n       LOYVAIN, CANONCOV.",
    "crumbs": [
      "Functions",
      "Coloyvain"
    ]
  },
  {
    "objectID": "functions/dispersion.html",
    "href": "functions/dispersion.html",
    "title": "abct",
    "section": "",
    "text": "DISPERSION Dispersion of network matrix\n\n   D = dispersion(W)\n   D = dispersion(W, type, M)\n\n   Inputs:\n       W: Network matrix of size n x n.\n\n       type: Dispersion type\n           \"coefvar2\": Squared coefficient of variation (default).\n           \"kpartcoef\": k-Participation coefficient.\n\n       M: Module vector of length n (if type is \"kpartcoef\" only).\n\n   Outputs:\n       D: Dispersion vector (length n).\n\n   Methodological notes:\n       The squared coefficient of variation, or CV2, is the ratio of the\n       variance to the square of the mean. CV2 is equivalent to the ratio\n       of the second moment to the square of the first moment.\n\n       The participation coefficient is a popular module-based measure of\n       connectional diversity. The k-participation coefficient is the\n       participation coefficient normalized by module size.\n\n       CV2 is approximately equivalent to the k-participation coefficient\n       in homogeneously modular networks, such as correlation or\n       co-neighbor networks.\n\n   See also:\n       DEGREE.",
    "crumbs": [
      "Functions",
      "Dispersion"
    ]
  },
  {
    "objectID": "functions/kneighbor.html",
    "href": "functions/kneighbor.html",
    "title": "abct",
    "section": "",
    "text": "KNEIGHBOR Common-neighbor or symmetric kappa-nearest-neighbor matrix\n\n   B = kneighbor(W)\n   B = kneighbor(W, type)\n   B = kneighbor(W, type, kappa)\n   B = kneighbor(X, type, kappa, similarity)\n   B = kneighbor(X, type, kappa, similarity, method)\n   B = kneighbor(X, type, kappa, similarity, method, Name=Value)\n\n   Inputs:\n\n       W: Network matrix of size n x n.\n       OR\n       X: Data matrix of size n x p, where\n           n is the number of data points and\n           p is the number of features.\n\n       type: Type of neighbor matrix.\n           \"common\": Common-neighbor matrix (default).\n           \"nearest\": Symmetric kappa-nearest neighbor matrix.\n\n       kappa: Number of nearest neighbors.\n           1 &lt;= kappa &lt; n (default is 10).\n           OR\n           0 &lt; kappa &lt; 1 to use as a fraction of n.\n\n       similarity: Type of similarity.\n           \"network\": Network connectivity (default).\n           \"corr\": Pearson correlation coefficient.\n           \"cosim\": Cosine similarity.\n\n       method: Method of neighbor search.\n           \"direct\": Direct computation of similarity matrix (default).\n           \"indirect\": knnsearch (in MATLAB)\n                       pynndescent (in Python).\n\n       Name=[Value] Arguments:\n           Optional arguments passed to knnsearch or pynndescent.\n\n   Outputs:\n       B: Co-neighbor or symmetric nearest-neighbor matrix (size n x n).\n\n   Methodological notes:\n       Symmetric kappa-nearest-neighbor matrices are binary matrices that\n       connect pairs of nodes if one of the nodes is a top-kappa nearest\n       neighbor of the other node (in a structural, correlation, or\n       another network).\n\n       kappa-common-neighbor matrices are symmetric integer matrices that\n       connect pairs of nodes by the number of their shared top-kappa\n       nearest neighbors.\n \n       Direct computation of the similarity matrix is performed in\n       blocks. It is generally faster than indirect computation.\n\n   Dependencies:\n       MATLAB: \n           Statistics and Machine Learning Toolbox (if method=\"indirect\")\n       Python: \n           PyNNDescent (if method=\"indirect\")\n\n   See also:\n       KNEICOMP, MUMAP.",
    "crumbs": [
      "Functions",
      "Kneighbor"
    ]
  },
  {
    "objectID": "functions/loyvain.html",
    "href": "functions/loyvain.html",
    "title": "abct",
    "section": "",
    "text": "LOYVAIN K-modularity, k-means, or spectral clustering\n\n   [M, Q] = loyvain(W, k)\n   [M, Q] = loyvain(W, k, objective)\n   [M, Q] = loyvain(X, k, objective, similarity)\n   [M, Q] = loyvain(_, objective, similarity, Name=Value)\n\n   Inputs:\n       W:  Network matrix of size n x n.\n       OR\n       X:  Data matrix of size n x p, where\n           n is the number of data points and\n           p is the number of features.\n\n       k: Number of modules (positive integer or 0).\n           Set to 0 to infer number from initial module assignment.\n\n       objective: Clustering objective.\n           \"kmodularity\": K-modularity (default).\n           \"kmeans\": K-means clustering objective.\n           \"spectral\": Spectral clustering objective (normalized cut).\n\n       similarity: Type of similarity.\n         The first option assumes that the first input is a network matrix.\n           \"network\": Network connectivity (default).\n               W is a symmetric network matrix. The network must\n               be non-negative for k-modularity and spectral\n               objectives. No additional similarity is computed.\n         The other options assume that the first input is X, a data matrix.\n           \"corr\": Pearson correlation coefficient.\n               A magnitude-normalized dot product of mean-centered vectors.\n           \"cosim\": Cosine similarity.\n               A normalized dot product.\n           \"cov\":  Covariance.\n               A dot product of mean-centered vectors.\n           \"dot\": Dot product.\n               A sum of an elementwise vector product.\n\n       Name=[Value] Arguments:\n\n           start=[Initial module assignments].\n               \"greedy\": Maximin (greedy kmeans++) initialization (default).\n               \"balanced\": Standard kmeans++ initialization.\n               \"random\": Uniformly random initialization.\n               Numeric vector: Initial module assignment vector of length n.\n\n           numbatches=[Number of batches].\n               Positive integer (default is 10).\n\n           maxiter=[Maximum number of algorithm iterations].\n               Positive integer (default is 1000).\n\n           replicates=[Number of replicates].\n               Positive integer (default is 10).\n\n           tolerance=[Convergence tolerance].\n               Positive scalar (default is 1e-10).\n\n           display=[Display progress].\n               \"none\": no display (default).\n               \"replicate\": display progress at each replicate.\n               \"iteration\": display progress at each iteration.\n\n   Outputs:\n       M: Vector of module assignments (length n).\n       Q: Value of maximized objective.\n\n   Methodological notes:\n       Loyvain is a unification of:\n       Lloyd's algorithm for k-means clustering and\n       Louvain algorithm for modularity maximization.\n\n       K-modularity maximization is exactly equivalent to normalized\n       modularity maximization and approximately equivalent to k-means\n       clustering with global residualization. Global residualization is\n       implemented as degree correction for network matrices and\n       global-signal regression for data matrices.\n\n       For \"network\" similarity, k-modularity is rescaled by:\n           (average module size) / (absolute sum of all weights)\n       This rescaling aligns k-modularity within the range of the\n       modularity, but has no effect on the optimization algorithm.\n\n       The Loyvain algorithm is not guaranteed to converge if\n       all swaps are accepted at each iteration (NumBatches = 1).\n       Therefore, it is generally a good idea to set NumBatches &gt; 1.\n\n   See also:\n       COLOYVAIN, CANONCOV, KNEICOMP, LOUVAINS, RESIDUALN.",
    "crumbs": [
      "Functions",
      "Loyvain"
    ]
  },
  {
    "objectID": "functions/residualn.html",
    "href": "functions/residualn.html",
    "title": "abct",
    "section": "",
    "text": "RESIDUALN Global residualization of network or data matrix\n\n   X1 = residualn(X)\n   X1 = residualn(X, type)\n\n   Inputs:\n       X:  Network matrix of size n x n, or data matrix of size n x p.\n           n is the number of nodes or data points and\n           p is the number of features.\n\n       type: Type of global residualization.\n           \"degree\": Degree correction (default).\n           \"global\": Global signal regression.\n           \"rankone\": Subtraction of rank-one approximation.\n\n   Outputs:\n       X1: Residual network or data matrix.\n\n   See also:\n       SHRINKAGE.",
    "crumbs": [
      "Functions",
      "Residualn"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "abct",
    "section": "",
    "text": "abct\nabct (abct.rubinovlab.net) is a MATLAB and Python toolbox for unsupervised learning, network science, and imaging/network neuroscience.\nThe toolbox includes three variants of global residualization, the Loyvain and co-Loyvain methods (for k-means, k-modularity, or spectral clustering of data or network inputs), as well as binary and weighted canonical and co-neighbor components, and m-umap embeddings. It also includes computation of degree centralities (first, second, and residual), dispersion centralities (squared coefficient of variation, k-participation coefficient), and network shrinkage.\nSee this reference for more details: Rubinov M. Unifying equivalences across unsupervised learning, network science, and imaging/network neuroscience. arXiv, 2025, doi:10.48550/arXiv.2508.10045.\n\nDownload\nABCT MATLAB ABCT Python GitHub\n\n\nInteractive examples\nThe examples illustrate the main analyses in the above study, and can be run interactively as online Jupyter notebooks in Google Colab (You need a Google account). To run in Colab, click the Open in Colab button at the top of the page of each example.\nOur example brain-imaging data come from the Human Connectome Project, a large brain-imaging resource.\nNB: For illustrative ease, some example analyses are simplified versions of analyses in the original study.\n\n\nContact\nMika Rubinov: mika.rubinov at vanderbilt.edu",
    "crumbs": [
      "Home"
    ]
  }
]